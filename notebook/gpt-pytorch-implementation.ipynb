{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset\n[truongpdd/vietnamese_poetry](https://huggingface.co/datasets/truongpdd/vietnamese_poetry)","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset('truongpdd/vietnamese_poetry')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-11T11:08:54.772301Z","iopub.execute_input":"2024-03-11T11:08:54.772824Z","iopub.status.idle":"2024-03-11T11:09:01.145561Z","shell.execute_reply.started":"2024-03-11T11:08:54.772792Z","shell.execute_reply":"2024-03-11T11:09:01.144739Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/717 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4591a058ab4f2c8b13148a47f9af6c"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset csv/default (download: 61.91 MiB, generated: 106.76 MiB, post-processed: Unknown size, total: 168.67 MiB) to /root/.cache/huggingface/datasets/parquet/truongpdd--vietnamese_poetry-a78eb18695f6197f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53014e44f8574b5686b9ac96ff7c8e3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/64.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d34100ce23458f877f3a61f75f6ca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4caef905374e4dd08ac2468882e59db2"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/truongpdd--vietnamese_poetry-a78eb18695f6197f/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701f70d803af4827a06300a1dbe01f9b"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:01.147433Z","iopub.execute_input":"2024-03-11T11:09:01.147882Z","iopub.status.idle":"2024-03-11T11:09:01.154387Z","shell.execute_reply.started":"2024-03-11T11:09:01.147857Z","shell.execute_reply":"2024-03-11T11:09:01.153420Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'the_loai'],\n        num_rows: 171188\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset['train'][0]['text'])\n# print('\\n'.join(dataset['train'][:2]['text']))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:01.155450Z","iopub.execute_input":"2024-03-11T11:09:01.155714Z","iopub.status.idle":"2024-03-11T11:09:01.167401Z","shell.execute_reply.started":"2024-03-11T11:09:01.155672Z","shell.execute_reply":"2024-03-11T11:09:01.166409Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"thÆ¡ lá»¥c bÃ¡t: \n ai Æ¡i xa báº¿n quÃª hÆ°Æ¡ng\nnhá»› vá» quÃª máº¹ náº¯ng sÆ°Æ¡ng Ä‘Ã¢y nÃ¨\nnhá»› sao nhá»¯ng buá»•i trÆ°a hÃ¨\nvÃµng Ä‘Æ°a cÃ³t kÃ©t giÃ³ hÃ¨ hiu hiu\nlá»i ru cá»§a máº¹ dáº¥u yÃªu\nngá»t ngÃ o Ãªm dá»‹u máº¹ yÃªu con nhiá»u\nlá»i máº¹ nÄƒm thÃ¡ng sá»›m chiá»u\ncon nghe nhá»› mÃ£i nhá»¯ng Ä‘iá»u ru ta\nlÃ²ng máº¹ Ã´i tháº­t bao la\nthÃ¡i bÃ¬nh rá»™ng lá»›n thÆ°Æ¡ng lÃ  mÃªnh mÃ´ng\nmá»™t Ä‘á»i lÆ°ng máº¹ Ä‘Ã£ cÃ²ng\nvai mang tay xÃ¡ch lo chá»“ng chÄƒm con\nmong sao con lá»›n nÃªn ngÆ°á»i\nthÃ¢n máº¹ cÃ³ cá»±c váº«n cÆ°á»i thÆ°Æ¡ng con\nvÃ¬ Ä‘á»i máº¹ sá»‘ng cho con\ngom bao má»‡t nhá»c mÃ£i cÃ²n vÃ²ng tay\nmong chá» cho Ä‘áº¿n má»™t ngÃ y\ncÃ´ng thÃ nh danh toáº¡i lÃ  ngÃ y máº¹ mong\ndÃ¹ nay máº¹ Ä‘Ã£ xa rá»“i\ncon Ä‘Ã¢y nhá»› mÃ£i máº¹ Ã´i váº¡n láº§n\nnguyá»‡n ráº±ng ghi nhá»› cÃ´ng Ã¢n\nsinh thÃ nh dÆ°á»¡ng dá»¥c máº«u thÃ¢n Ä‘á»i Ä‘á»i\n","output_type":"stream"}]},{"cell_type":"code","source":"text = '\\n'.join(dataset['train'][:]['text'])\nlen(text)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:01.168590Z","iopub.execute_input":"2024-03-11T11:09:01.168958Z","iopub.status.idle":"2024-03-11T11:09:02.517471Z","shell.execute_reply.started":"2024-03-11T11:09:01.168926Z","shell.execute_reply":"2024-03-11T11:09:02.516520Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"83454368"},"metadata":{}}]},{"cell_type":"code","source":"print(text[:100])","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:02.520481Z","iopub.execute_input":"2024-03-11T11:09:02.520812Z","iopub.status.idle":"2024-03-11T11:09:02.525341Z","shell.execute_reply.started":"2024-03-11T11:09:02.520788Z","shell.execute_reply":"2024-03-11T11:09:02.524475Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"thÆ¡ lá»¥c bÃ¡t: \n ai Æ¡i xa báº¿n quÃª hÆ°Æ¡ng\nnhá»› vá» quÃª máº¹ náº¯ng sÆ°Æ¡ng Ä‘Ã¢y nÃ¨\nnhá»› sao nhá»¯ng buá»•i trÆ°a hÃ¨\nvÃµn\n","output_type":"stream"}]},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('chars:',chars)\nprint('vocab_size:', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:02.526658Z","iopub.execute_input":"2024-03-11T11:09:02.527292Z","iopub.status.idle":"2024-03-11T11:09:04.382263Z","shell.execute_reply.started":"2024-03-11T11:09:02.527260Z","shell.execute_reply":"2024-03-11T11:09:04.381232Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"chars: ['\\x08', '\\n', ' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '=', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', 'Â¡', 'Â¢', 'Â¤', 'Â¥', 'Â¬', '\\xad', 'Â°', 'Â²', 'Â³', 'Â´', 'Âµ', 'Â¸', 'Â¹', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¿', 'Ä', 'Äƒ', 'Ä…', 'Ä‡', 'Ä‘', 'Ä“', 'Ä™', 'Ä›', 'Ä©', 'Ä«', 'Å„', 'Å', 'Å“', 'Å¡', 'Å¥', 'Å©', 'Å«', 'Å¯', 'Æ¡', 'Æ£', 'Æ°', 'ÇŽ', 'Ç’', 'Çš', 'Çµ', 'Ç¹', 'Ì€', 'Ì', 'Ì‚', 'Ìƒ', 'Ì‰', 'Ì£', 'Î³', 'Ñ', 'Ñ†', 'á¸¥', 'á¸·', 'á¸¿', 'á¹ƒ', 'á¹‡', 'á¹•', 'á¹­', 'áº¡', 'áº£', 'áº¥', 'áº§', 'áº©', 'áº«', 'áº­', 'áº¯', 'áº±', 'áº³', 'áºµ', 'áº·', 'áº¹', 'áº»', 'áº½', 'áº¿', 'á»', 'á»ƒ', 'á»…', 'á»‡', 'á»‰', 'á»‹', 'á»', 'á»', 'á»‘', 'á»“', 'á»•', 'á»—', 'á»™', 'á»›', 'á»', 'á»Ÿ', 'á»¡', 'á»£', 'á»¥', 'á»§', 'á»©', 'á»«', 'á»­', 'á»¯', 'á»±', 'á»³', 'á»µ', 'á»·', 'á»¹', '\\u200b', '\\u200c', '\\u200d', '\\u200e', 'â‚«', 'â—‡', 'â„', 'â£', 'â¤', 'ã€‚', 'ï¸', '\\ufeff', 'ï¼Œ', 'ï¿¼', 'ðŸ', 'ðŸ‚', 'ðŸ’‹', 'ðŸ˜‰', 'ðŸ˜”', 'ðŸ˜¥', 'ðŸ˜§', 'ðŸ¤£']\nvocab_size: 194\n","output_type":"stream"}]},{"cell_type":"code","source":"str2int = {char:i for i, char in enumerate(chars)}\nint2str = {i:char for i, char in enumerate(chars)}\nencode = lambda s: [str2int[c] for c in s]\ndecode = lambda lst: ''.join([int2str[i] for i in lst])\nprint(encode('nguyá»…n gia bÃ¬nh'))\nprint(decode(encode('nguyá»…n gia bÃ¬nh')))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:04.383491Z","iopub.execute_input":"2024-03-11T11:09:04.383827Z","iopub.status.idle":"2024-03-11T11:09:04.393633Z","shell.execute_reply.started":"2024-03-11T11:09:04.383800Z","shell.execute_reply":"2024-03-11T11:09:04.392578Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[29, 22, 36, 40, 145, 29, 2, 22, 24, 16, 2, 17, 68, 29, 23]\nnguyá»…n gia bÃ¬nh\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndata = torch.tensor(encode(text), dtype=torch.long)\ndata.shape, data.dtype, data[:100]","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:04.394834Z","iopub.execute_input":"2024-03-11T11:09:04.395655Z","iopub.status.idle":"2024-03-11T11:09:26.312108Z","shell.execute_reply.started":"2024-03-11T11:09:04.395617Z","shell.execute_reply":"2024-03-11T11:09:26.311032Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(torch.Size([83454368]),\n torch.int64,\n tensor([ 35,  23, 103,   2,  27, 161,  18,   2,  17,  58,  35,  13,   2,   1,\n           2,  16,  24,   2, 103,  24,   2,  39,  16,   2,  17, 142,  29,   2,\n          32,  36,  66,   2,  23, 105, 103,  29,  22,   1,  29,  23, 156,   2,\n          37, 143,   2,  32,  36,  66,   2,  28, 139,   2,  29, 134,  29,  22,\n           2,  34, 105, 103,  29,  22,   2,  89,  59,  40,   2,  29,  64,   1,\n          29,  23, 156,   2,  34,  16,  30,   2,  29,  23, 166,  29,  22,   2,\n          17,  36, 153,  24,   2,  35,  33, 105,  16,   2,  23,  64,   1,  37,\n          76,  29]))"},"metadata":{}}]},{"cell_type":"code","source":"n = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:26.313347Z","iopub.execute_input":"2024-03-11T11:09:26.314688Z","iopub.status.idle":"2024-03-11T11:09:26.319519Z","shell.execute_reply.started":"2024-03-11T11:09:26.314651Z","shell.execute_reply":"2024-03-11T11:09:26.318643Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"block_size = 8\nx_train = train_data[:block_size]\ny_train = train_data[1:block_size+1]\nfor i in range(block_size):\n    print(f'context: {x_train[:i+1]} => target: {y_train[i]}')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:09:26.321292Z","iopub.execute_input":"2024-03-11T11:09:26.321696Z","iopub.status.idle":"2024-03-11T11:09:26.342902Z","shell.execute_reply.started":"2024-03-11T11:09:26.321666Z","shell.execute_reply":"2024-03-11T11:09:26.341932Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"context: tensor([35]) => target: 23\ncontext: tensor([35, 23]) => target: 103\ncontext: tensor([ 35,  23, 103]) => target: 2\ncontext: tensor([ 35,  23, 103,   2]) => target: 27\ncontext: tensor([ 35,  23, 103,   2,  27]) => target: 161\ncontext: tensor([ 35,  23, 103,   2,  27, 161]) => target: 18\ncontext: tensor([ 35,  23, 103,   2,  27, 161,  18]) => target: 2\ncontext: tensor([ 35,  23, 103,   2,  27, 161,  18,   2]) => target: 17\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(42)\nbatch_size = 32\nblock_size = 8\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndef get_batch(split):\n    data = train_data if split=='train' else val_data\n    start_idx = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in start_idx])\n    y = torch.stack([data[i+block_size:i+block_size+1] for i in start_idx])\n    x = x.to(device)\n    y = y.to(device)\n    return x, y\n\n# xb, yb = get_batch('train')\n# print('inputs:')\n# print(xb.shape)\n# print(xb)\n# print('targets:')\n# print(yb.shape)\n# print(yb)","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:53:27.663640Z","iopub.execute_input":"2024-03-11T11:53:27.664055Z","iopub.status.idle":"2024-03-11T11:53:27.672826Z","shell.execute_reply.started":"2024-03-11T11:53:27.664026Z","shell.execute_reply":"2024-03-11T11:53:27.671636Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"batch_size = 4\nblock_size = 32\nix = torch.randint(len(data) - block_size, (batch_size,))\nix","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:53:28.142357Z","iopub.execute_input":"2024-03-11T11:53:28.143231Z","iopub.status.idle":"2024-03-11T11:53:28.150093Z","shell.execute_reply.started":"2024-03-11T11:53:28.143197Z","shell.execute_reply":"2024-03-11T11:53:28.149195Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"tensor([23005158, 82952627, 77478748, 36757390])"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(42)\nB, T, C = 32, 8, 32 # batch, time, channel\nx = torch.rand(B, T, C)\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)\nq = query(x)\nW = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\ntril = torch.tril(torch.ones(T, T))\nW = W.masked_fill(tril==0, float('-inf'))\nW = F.softmax(W, dim=-1)\nv = value(x) # (B, T, 16)\nout = W @ v # (B, T, T) @ (B, T, 16) > (B, T, 16)\nW[0]","metadata":{"execution":{"iopub.status.busy":"2024-03-11T11:53:28.449910Z","iopub.execute_input":"2024-03-11T11:53:28.450559Z","iopub.status.idle":"2024-03-11T11:53:28.464874Z","shell.execute_reply.started":"2024-03-11T11:53:28.450526Z","shell.execute_reply":"2024-03-11T11:53:28.463975Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4513, 0.5487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2947, 0.2783, 0.4270, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2191, 0.2147, 0.3137, 0.2525, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1967, 0.1663, 0.2031, 0.2164, 0.2174, 0.0000, 0.0000, 0.0000],\n        [0.1449, 0.1583, 0.1539, 0.1774, 0.1584, 0.2071, 0.0000, 0.0000],\n        [0.1366, 0.1427, 0.1384, 0.1606, 0.1536, 0.1445, 0.1236, 0.0000],\n        [0.1170, 0.1169, 0.0999, 0.1216, 0.1214, 0.1216, 0.1333, 0.1683]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        B, T, C  = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        wei = q @ k.transpose(-2, -1)/(C**0.5) #  (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei)\n        v = self.value(x)\n        output = wei @ v # (B, T, T) @ (B, T ,C) > (B, T, C)\n        return output\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        x = torch.cat([h(x) for h in self.heads], dim=-1)\n        x = self.proj(x)\n        x = self.dropout(x)\n        return x\n\nclass FeedForward(nn.Module):\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4*n_embd),\n            nn.ReLU(),\n            nn.Linear(4*n_embd, n_embd),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n    \n\nclass DecoderBlock(nn.Module):\n    def __init__(self, n_embd, n_heads):\n        super().__init__()\n        head_size = n_embd//n_heads\n        self.sa = MultiHeadAttention(n_heads, head_size)\n        self.ff = FeedForward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n    \n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ff(self.ln2(x))\n        return x\n\nclass GPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[DecoderBlock(n_embd, n_heads=n_heads) for _ in range(n_layers)])\n        self.ln = nn.LayerNorm(n_embd)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n    \n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        token_embd = self.token_embedding(idx)\n        position_embd = self.position_embedding(torch.arange(T, device=device))\n        x = token_embd + position_embd\n        x = self.blocks(x)\n        x = self.ln(x)\n        logits = self.lm_head(x)\n        \n        if targets is not None:\n            B, T, C = logits.shape\n#             print(logits.shape, targets.shape)\n            logits = logits.view(B*T, C)\n            \n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        else:\n            loss = None\n        return logits, loss\n    \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            next_idx = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, next_idx), dim=1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:31:07.419205Z","iopub.execute_input":"2024-03-11T12:31:07.420317Z","iopub.status.idle":"2024-03-11T12:31:07.442095Z","shell.execute_reply.started":"2024-03-11T12:31:07.420281Z","shell.execute_reply":"2024-03-11T12:31:07.441180Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters).to(device)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:31:07.443600Z","iopub.execute_input":"2024-03-11T12:31:07.444134Z","iopub.status.idle":"2024-03-11T12:31:07.458830Z","shell.execute_reply.started":"2024-03-11T12:31:07.444109Z","shell.execute_reply":"2024-03-11T12:31:07.457960Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"import os\nfrom torch.optim import lr_scheduler\n\nbatch_size = 128 \nblock_size = 32 \nmax_iters = 20000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_heads = 4\nn_layers = 4\ndropout = 0.0\nbest_loss = 100000\n\nmodel = GPT()\nmodel = model.to(device)\nif 'last_GPT.pt' in os.listdir('.'):\n    model.load_state_dict(torch.load('last_GPT.pt', map_location=torch.device('cpu')))\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.3)\nprint('model has', sum(p.numel() for p in model.parameters())/1e6, 'M parameters')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T13:10:05.229582Z","iopub.execute_input":"2024-03-11T13:10:05.229936Z","iopub.status.idle":"2024-03-11T13:10:05.270066Z","shell.execute_reply.started":"2024-03-11T13:10:05.229910Z","shell.execute_reply":"2024-03-11T13:10:05.269116Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"model has 0.22637 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_batch(split):\n    data = train_data if split=='train' else val_data\n    start_idx = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in start_idx])\n    y = torch.stack([data[i+1:i+block_size+1] for i in start_idx])\n    x = x.to(device)\n    y = y.to(device)\n    return x, y\nx, y =get_batch('train')\ny.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-11T13:10:05.690061Z","iopub.execute_input":"2024-03-11T13:10:05.690909Z","iopub.status.idle":"2024-03-11T13:10:05.706018Z","shell.execute_reply.started":"2024-03-11T13:10:05.690876Z","shell.execute_reply":"2024-03-11T13:10:05.704974Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"torch.Size([128, 32])"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\nfor i in range(max_iters):\n    if i % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        if losses['val'] < best_loss:\n            best_loss = losses['val']\n            torch.save(model.state_dict(), 'best_GPT.pt')   \n        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        \n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    torch.save(model.state_dict(), 'last_GPT.pt') \n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-11T13:10:06.199288Z","iopub.execute_input":"2024-03-11T13:10:06.199629Z","iopub.status.idle":"2024-03-11T13:22:12.242476Z","shell.execute_reply.started":"2024-03-11T13:10:06.199603Z","shell.execute_reply":"2024-03-11T13:22:12.240895Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"step 0: train loss 1.5937, val loss 1.6580\nstep 100: train loss 1.6105, val loss 1.6752\nstep 200: train loss 1.6057, val loss 1.6663\nstep 300: train loss 1.6077, val loss 1.6586\nstep 400: train loss 1.6055, val loss 1.6702\nstep 500: train loss 1.6038, val loss 1.6653\nstep 600: train loss 1.6011, val loss 1.6686\nstep 700: train loss 1.6036, val loss 1.6626\nstep 800: train loss 1.6021, val loss 1.6545\nstep 900: train loss 1.6001, val loss 1.6618\nstep 1000: train loss 1.5952, val loss 1.6576\nstep 1100: train loss 1.5826, val loss 1.6493\nstep 1200: train loss 1.5824, val loss 1.6490\nstep 1300: train loss 1.5795, val loss 1.6464\nstep 1400: train loss 1.5809, val loss 1.6423\nstep 1500: train loss 1.5782, val loss 1.6465\nstep 1600: train loss 1.5768, val loss 1.6495\nstep 1700: train loss 1.5770, val loss 1.6425\nstep 1800: train loss 1.5780, val loss 1.6437\nstep 1900: train loss 1.5772, val loss 1.6486\nstep 2000: train loss 1.5746, val loss 1.6447\nstep 2100: train loss 1.5718, val loss 1.6434\nstep 2200: train loss 1.5713, val loss 1.6389\nstep 2300: train loss 1.5706, val loss 1.6431\nstep 2400: train loss 1.5722, val loss 1.6392\nstep 2500: train loss 1.5661, val loss 1.6362\nstep 2600: train loss 1.5697, val loss 1.6379\nstep 2700: train loss 1.5688, val loss 1.6353\nstep 2800: train loss 1.5652, val loss 1.6356\nstep 2900: train loss 1.5715, val loss 1.6380\nstep 3000: train loss 1.5683, val loss 1.6363\nstep 3100: train loss 1.5684, val loss 1.6367\nstep 3200: train loss 1.5651, val loss 1.6358\nstep 3300: train loss 1.5692, val loss 1.6349\nstep 3400: train loss 1.5653, val loss 1.6362\nstep 3500: train loss 1.5669, val loss 1.6421\nstep 3600: train loss 1.5671, val loss 1.6359\nstep 3700: train loss 1.5645, val loss 1.6353\nstep 3800: train loss 1.5642, val loss 1.6375\nstep 3900: train loss 1.5675, val loss 1.6352\nstep 4000: train loss 1.5661, val loss 1.6343\nstep 4100: train loss 1.5666, val loss 1.6385\nstep 4200: train loss 1.5675, val loss 1.6340\nstep 4300: train loss 1.5665, val loss 1.6375\nstep 4400: train loss 1.5671, val loss 1.6345\nstep 4500: train loss 1.5664, val loss 1.6346\nstep 4600: train loss 1.5644, val loss 1.6343\nstep 4700: train loss 1.5650, val loss 1.6374\nstep 4800: train loss 1.5630, val loss 1.6346\nstep 4900: train loss 1.5648, val loss 1.6386\nstep 5000: train loss 1.5635, val loss 1.6344\nstep 5100: train loss 1.5622, val loss 1.6340\nstep 5200: train loss 1.5650, val loss 1.6383\nstep 5300: train loss 1.5642, val loss 1.6347\nstep 5400: train loss 1.5627, val loss 1.6345\nstep 5500: train loss 1.5661, val loss 1.6316\nstep 5600: train loss 1.5606, val loss 1.6332\nstep 5700: train loss 1.5650, val loss 1.6335\nstep 5800: train loss 1.5618, val loss 1.6321\nstep 5900: train loss 1.5655, val loss 1.6377\nstep 6000: train loss 1.5620, val loss 1.6363\nstep 6100: train loss 1.5655, val loss 1.6310\nstep 6200: train loss 1.5671, val loss 1.6350\nstep 6300: train loss 1.5617, val loss 1.6386\nstep 6400: train loss 1.5639, val loss 1.6365\nstep 6500: train loss 1.5628, val loss 1.6369\nstep 6600: train loss 1.5650, val loss 1.6346\nstep 6700: train loss 1.5653, val loss 1.6349\nstep 6800: train loss 1.5626, val loss 1.6335\nstep 6900: train loss 1.5645, val loss 1.6355\nstep 7000: train loss 1.5668, val loss 1.6359\nstep 7100: train loss 1.5652, val loss 1.6347\nstep 7200: train loss 1.5642, val loss 1.6365\nstep 7300: train loss 1.5632, val loss 1.6366\nstep 7400: train loss 1.5639, val loss 1.6353\nstep 7500: train loss 1.5666, val loss 1.6400\nstep 7600: train loss 1.5638, val loss 1.6360\nstep 7700: train loss 1.5676, val loss 1.6332\nstep 7800: train loss 1.5655, val loss 1.6342\nstep 7900: train loss 1.5625, val loss 1.6364\nstep 8000: train loss 1.5642, val loss 1.6352\nstep 8100: train loss 1.5647, val loss 1.6341\nstep 8200: train loss 1.5658, val loss 1.6367\nstep 8300: train loss 1.5641, val loss 1.6339\nstep 8400: train loss 1.5606, val loss 1.6338\nstep 8500: train loss 1.5655, val loss 1.6300\nstep 8600: train loss 1.5635, val loss 1.6385\nstep 8700: train loss 1.5670, val loss 1.6355\nstep 8800: train loss 1.5640, val loss 1.6344\nstep 8900: train loss 1.5633, val loss 1.6328\nstep 9000: train loss 1.5628, val loss 1.6336\nstep 9100: train loss 1.5624, val loss 1.6351\nstep 9200: train loss 1.5641, val loss 1.6352\nstep 9300: train loss 1.5643, val loss 1.6338\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[123], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m best_loss:\n\u001b[1;32m      6\u001b[0m             best_loss \u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[94], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m      8\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[0;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     11\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[93], line 76\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m position_embd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m token_embd \u001b[38;5;241m+\u001b[39m position_embd\n\u001b[0;32m---> 76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(x)\n\u001b[1;32m     78\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[93], line 58\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 58\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[93], line 30\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n","Cell \u001b[0;32mIn[93], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[93], line 13\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m B, T, C  \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     12\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x)\n\u001b[0;32m---> 13\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m(C\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;66;03m#  (B, T, C) @ (B, C, T) -> (B, T, T)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=device)\ntext = 'nhá»› em'\ncontext = torch.asarray([encode(text)], dtype=torch.long, device=device)\nprint( context)\nprint((decode(con) for con in context))\nprint(decode(model.generate(context, max_new_tokens=200)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T13:34:46.059769Z","iopub.execute_input":"2024-03-11T13:34:46.060130Z","iopub.status.idle":"2024-03-11T13:34:47.577368Z","shell.execute_reply.started":"2024-03-11T13:34:46.060103Z","shell.execute_reply":"2024-03-11T13:34:47.576416Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"tensor([[ 29,  23, 156,   2,  20,  28]], device='cuda:0')\n<generator object <genexpr> at 0x7d66b8c88c10>\nnhá»› em cÃ²n mÃ¢y\ngiáº¥u dáº«u cháº¯c cÅ©ng soi nhÃ²a Ä‘áº¥t nÆ°á»›c\ntráº» quÃª chÃ¹a Ä‘Ãªm Ä‘en sÃ´ng quÃ¡ há»“ng\nngÃ³ ngÃ o quÃªn dÃ¡nh vÅ©ng náº¥c vá»i\nngÃ y xÆ°a vá» áº§m chÃºm ngá»t ngÃ£\nÄ‘á»ƒ lÃ²ng ngáº¯m trong há»“n nhung Æ°Æ¡m con\nta trá»Ÿ Ä‘Ã² trá»n phá»‘ ph\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'GPT.pt')","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:41:32.644449Z","iopub.execute_input":"2024-03-11T12:41:32.644839Z","iopub.status.idle":"2024-03-11T12:41:32.664909Z","shell.execute_reply.started":"2024-03-11T12:41:32.644808Z","shell.execute_reply":"2024-03-11T12:41:32.663946Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"load_model = GPT().to(device)\nload_model.load_state_dict(torch.load('GPT.pt', map_location=torch.device('cpu')))\nlaod_model.eval()\nprint(decode(load_model.generate(context, max_new_tokens=200)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-03-11T12:43:09.290796Z","iopub.execute_input":"2024-03-11T12:43:09.291154Z","iopub.status.idle":"2024-03-11T12:43:10.818136Z","shell.execute_reply.started":"2024-03-11T12:43:09.291128Z","shell.execute_reply":"2024-03-11T12:43:10.817158Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"\b áº£i Ä‘Ã£ xa\n\nbÃªn sá»± anh máº¯t tá»«ng lá» biá»c vá»›i xuÃ¢n tá»«ng sa\nnhÃ¬n thÆ¡ xanh vá»›i mÃ¬nh mÃ¢y\ntrÃ²n nÆ¡i lÆ°á»£n cÃ¡i lÃ²ng chÆ°a vá»i\ncÃ¡i gÆ°Æ¡ ngoÃ i thÃ¡ng mang ná»¥ cÆ°á»i\nsáº§u bÃ¬nh trá»i má»¹ Ä‘Ãªm quáº­n thÆ°Æ¡ng\ngiá»¯ lÃ²ng soi tháº©n b\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}